{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":492536,"sourceType":"datasetVersion","datasetId":230545}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\ndata_path = \"/kaggle/input/shanghaitech/\"\nprint(os.listdir(data_path))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:25:38.296352Z","iopub.execute_input":"2025-10-24T07:25:38.296847Z","iopub.status.idle":"2025-10-24T07:25:38.318632Z","shell.execute_reply.started":"2025-10-24T07:25:38.296820Z","shell.execute_reply":"2025-10-24T07:25:38.317631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importing required libraries.\n\n#GENERAL\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\n#PATH PROCESS\nimport os\nfrom pathlib import Path\nimport glob\nfrom scipy.io import loadmat\n\n# image processing\nimport cv2\nfrom scipy.ndimage import gaussian_filter\n\n# Neural Network\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:25:38.319486Z","iopub.execute_input":"2025-10-24T07:25:38.319805Z","iopub.status.idle":"2025-10-24T07:25:47.045551Z","shell.execute_reply.started":"2025-10-24T07:25:38.319782Z","shell.execute_reply":"2025-10-24T07:25:47.044943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Correct Windows path using raw string\npath_img_sample = r\"/kaggle/input/shanghaitech/ShanghaiTech/part_B/train_data/images/IMG_115.jpg\"\n\np = Path(path_img_sample)\nif not p.exists():\n    raise FileNotFoundError(f\"Image not found: {p.resolve()}\")\n\n# Normal read\nimg = cv2.imread(str(p), cv2.IMREAD_COLOR)\n# Fallback for Windows unicode paths\nif img is None:\n    data = np.fromfile(str(p), dtype=np.uint8)\n    if data.size:\n        img = cv2.imdecode(data, cv2.IMREAD_COLOR)\n\nif img is None:\n    raise ValueError(f\"Failed to read image: {p}\")\n\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n# Display\nplt.imshow(img_rgb)\nplt.title(\"Sample Crowd Image\")\nplt.axis('off')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:25:47.046857Z","iopub.execute_input":"2025-10-24T07:25:47.047216Z","iopub.status.idle":"2025-10-24T07:25:47.462970Z","shell.execute_reply.started":"2025-10-24T07:25:47.047199Z","shell.execute_reply":"2025-10-24T07:25:47.462178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ground-truth corrosponding to the sample image.\n# Prefer the ground-truth folder next to the images folder\ntarget_filename = \"GT_IMG_37.mat\"\ngt_candidate = p.parent.parent / \"ground-truth\" / target_filename\n\nif gt_candidate.exists():\n\tgt_path = str(gt_candidate)\nelse:\n\t# Try GT file that matches the sample image (e.g. IMG_37 -> GT_IMG_37.mat)\n\tgt_for_img_name = f\"GT_{p.stem}.mat\"\n\tgt_candidate2 = p.parent.parent / \"ground-truth\" / gt_for_img_name\n\tif gt_candidate2.exists():\n\t\tgt_path = str(gt_candidate2)\n\t\tprint(f\"Requested GT not found. Using GT for sample image: {gt_path}\")\n\telse:\n\t\tgt_dir = p.parent.parent / \"ground-truth\"\n\t\tif gt_dir.exists():\n\t\t\tavailable = sorted(gt_dir.glob(\"GT_*.mat\"))\n\t\t\tif available:\n\t\t\t\tgt_path = str(available[0])\n\t\t\t\tprint(f\"Requested GT not found. Using first available GT file: {gt_path}\")\n\t\t\telse:\n\t\t\t\traise FileNotFoundError(f\"No GT .mat files found in {gt_dir}\")\n\t\telse:\n\t\t\traise FileNotFoundError(\n\t\t\t\tf\"Ground-truth directory not found: {gt_dir}\\n\"\n\t\t\t\tf\"Tried: {gt_candidate} and {gt_candidate2}\"\n\t\t\t)\n\ngt_sample = loadmat(gt_path)\nprint('Loaded:', gt_path)\nprint('type:', type(gt_sample))\nprint(list(gt_sample.keys()))\n# show a short preview instead of dumping all items\nitems_preview = list(gt_sample.items())[:5]\nprint(\"Items preview:\", items_preview)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:25:47.465903Z","iopub.execute_input":"2025-10-24T07:25:47.466103Z","iopub.status.idle":"2025-10-24T07:25:47.487157Z","shell.execute_reply.started":"2025-10-24T07:25:47.466087Z","shell.execute_reply":"2025-10-24T07:25:47.486434Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(gt_sample.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:25:47.487908Z","iopub.execute_input":"2025-10-24T07:25:47.488116Z","iopub.status.idle":"2025-10-24T07:25:47.493271Z","shell.execute_reply.started":"2025-10-24T07:25:47.488100Z","shell.execute_reply":"2025-10-24T07:25:47.492681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting the coordinates from the ground-truth sample.\n\ngt_coor_sample = gt_sample.get('image_info')[0][0][0][0][0]\nprint('Shape of coordinates: ', gt_coor_sample.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:25:47.493791Z","iopub.execute_input":"2025-10-24T07:25:47.493979Z","iopub.status.idle":"2025-10-24T07:25:47.504853Z","shell.execute_reply.started":"2025-10-24T07:25:47.493965Z","shell.execute_reply":"2025-10-24T07:25:47.504297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Marking the coordinates after extracting them from the ground-truth on the original image sample.\nfigure = plt.figure(figsize=(5,5))\n\n# create a copy of the RGB image loaded previously and draw markers on that copy\nimage_sample = img_rgb.copy()\n\nfor x_cor, y_cor in gt_coor_sample:\n    cv2.drawMarker(image_sample, (int(x_cor), int(y_cor)), (255, 0, 0), markerType=cv2.MARKER_CROSS, thickness=3, markerSize=10)\n\nplt.imshow(image_sample)\nplt.title(\"Image and Coordinate\")\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:25:47.505563Z","iopub.execute_input":"2025-10-24T07:25:47.505850Z","iopub.status.idle":"2025-10-24T07:25:47.733641Z","shell.execute_reply.started":"2025-10-24T07:25:47.505834Z","shell.execute_reply":"2025-10-24T07:25:47.732755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gen_density_map_gaussian(image, coords, sigma=5):\n    img_zeros = np.zeros((image.shape[:2]), dtype=np.float32)\n    for x_cor, y_cor in coords:\n        img_zeros[int(y_cor), int(x_cor)] = 1\n\n    density_map = gaussian_filter(img_zeros,sigma=sigma,truncate=5*5)\n\n    return density_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:25:47.735908Z","iopub.execute_input":"2025-10-24T07:25:47.736151Z","iopub.status.idle":"2025-10-24T07:25:47.740959Z","shell.execute_reply.started":"2025-10-24T07:25:47.736133Z","shell.execute_reply":"2025-10-24T07:25:47.740077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"density_map_sample = gen_density_map_gaussian(image_sample, gt_coor_sample, 5)\n\n# Creating a new Matplotlib figure.\nfigure = plt.figure(figsize=(10,5))\n\nplt.subplot(1,2,1)\n\n# Converting image into torch tensor.\nimage_sample = torch.tensor(image_sample/255, dtype=torch.float)\nplt.xlabel(image_sample.shape)\nplt.title('GT: '+str(gt_coor_sample.shape[0]))\n\n# Showing the original image with the ground-truth marks.\nplt.imshow(image_sample)\n\nplt.subplot(1,2,2)\nplt.xlabel(density_map_sample.shape)\nplt.title('DM: '+str(np.sum(density_map_sample)))\n\n# Showing the corrosponding generated density map.\nplt.imshow(density_map_sample, cmap=\"jet\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:25:47.742017Z","iopub.execute_input":"2025-10-24T07:25:47.742473Z","iopub.status.idle":"2025-10-24T07:25:48.640850Z","shell.execute_reply.started":"2025-10-24T07:25:47.742454Z","shell.execute_reply":"2025-10-24T07:25:48.640216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass CrowdDataset(Dataset):\n    def __init__(self, root_dir, gt_downsample=4, shuffle=False):\n        self.root_dir = root_dir\n        self.gt_downsample = gt_downsample\n        self.shuffle = shuffle\n\n\n        self.img_names = [filename for filename in os.listdir(os.path.join(root_dir, 'images')) if filename.endswith('.jpg')]\n\n        if self.shuffle:\n            random.shuffle(self.img_names)\n\n        self.n_people = {}\n        self.DMs = {}\n        for image_filename in self.img_names:\n            img_path = os.path.join(root_dir, 'images', image_filename)\n            GT_filename = 'GT_' + image_filename.split('.')[0] + '.mat'\n            path_GT = os.path.join(root_dir, 'ground-truth', GT_filename)\n            GT = loadmat(path_GT).get('image_info')[0][0][0][0][0]\n            img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n            self.DMs[img_path] = gen_density_map_gaussian(img, GT, 5)\n            self.n_people[img_path] = GT.shape[0]\n\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.root_dir, 'images', self.img_names[index])  # Include the directory path\n        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n        gt_density_map = self.DMs[img_path]\n        gt_n_people = self.n_people[img_path]\n\n        if len(img.shape) == 2: \n            img = img[:, :, np.newaxis]\n            img = np.concatenate((img, img, img), 2)\n\n        # downsample\n        ds_rows = int(img.shape[0] // self.gt_downsample)\n        ds_cols = int(img.shape[1] // self.gt_downsample)\n        img = cv2.resize(img, (ds_cols*self.gt_downsample, ds_rows*self.gt_downsample))\n        gt_density_map = cv2.resize(gt_density_map, (ds_cols, ds_rows))\n        gt_density_map = gt_density_map[np.newaxis, :, :] * self.gt_downsample * self.gt_downsample\n\n        img = img.transpose((2,0,1)) # convert to order (channel, rows, cols)\n        img_tensor = torch.tensor(img/255, dtype=torch.float)\n        dm_tensor = torch.tensor(gt_density_map, dtype=torch.float)\n\n        return img_tensor, dm_tensor, gt_n_people","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:25:48.641720Z","iopub.execute_input":"2025-10-24T07:25:48.642033Z","iopub.status.idle":"2025-10-24T07:25:48.654641Z","shell.execute_reply.started":"2025-10-24T07:25:48.642014Z","shell.execute_reply":"2025-10-24T07:25:48.653913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split\n\nroot_dir = \"../input/shanghaitech/ShanghaiTech/part_B/test_data/\"\ndataset = CrowdDataset(root_dir, gt_downsample=4, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:25:48.655364Z","iopub.execute_input":"2025-10-24T07:25:48.655582Z","iopub.status.idle":"2025-10-24T07:27:15.625520Z","shell.execute_reply.started":"2025-10-24T07:25:48.655566Z","shell.execute_reply":"2025-10-24T07:27:15.624877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print some samples of dataset as a sanity check\nfor i, (img, gt_dmap, n_people) in enumerate(dataset):\n  plt.figure(figsize=(10, 5))\n  plt.subplot(1,2,1)\n  plt.xlabel(img.shape)\n  plt.title('GT: ' + str(n_people))\n  plt.imshow(img.permute(1, 2, 0))\n\n  plt.subplot(1,2,2)\n  plt.xlabel(gt_dmap.shape)\n  plt.title('DM: ' + str(np.sum(gt_dmap.numpy())))\n  plt.imshow(gt_dmap.permute(1, 2, 0), cmap=\"jet\")\n  plt.show()\n\n  if i > 0:\n    print('type of img: ', type(img))\n    print('type of dmap: ', type(gt_dmap))\n    print('shape of img: ', img.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:27:15.626298Z","iopub.execute_input":"2025-10-24T07:27:15.626544Z","iopub.status.idle":"2025-10-24T07:27:16.526898Z","shell.execute_reply.started":"2025-10-24T07:27:15.626523Z","shell.execute_reply":"2025-10-24T07:27:16.526049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MC_CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.column1 = nn.Sequential(\n            nn.Conv2d(3, 8, 9, padding='same'),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(8, 16, 7, padding='same'),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, 7, padding='same'),\n            nn.ReLU(),\n            nn.Conv2d(32, 16, 7, padding='same'),\n            nn.ReLU(),\n            nn.Conv2d(16, 8, 7, padding='same'),\n            nn.ReLU(),\n        )\n\n        self.column2 = nn.Sequential(\n            nn.Conv2d(3, 10, 7,padding='same'),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(10, 20, 5,padding='same'),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(20, 40, 5,padding='same'),\n            nn.ReLU(),\n            nn.Conv2d(40, 20, 5,padding='same'),\n            nn.ReLU(),\n            nn.Conv2d(20, 10, 5,padding='same'),\n            nn.ReLU(),\n        )\n\n        self.column3 = nn.Sequential(\n            nn.Conv2d(3, 12, 5, padding='same'),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(12, 24, 3, padding='same'),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(24, 48, 3, padding='same'),\n            nn.ReLU(),\n            nn.Conv2d(48, 24, 3, padding='same'),\n            nn.ReLU(),\n            nn.Conv2d(24, 12, 3, padding='same'),\n            nn.ReLU(),\n        )\n        \n\n        self.fusion_layer = nn.Sequential(\n            nn.Conv2d(30, 1, 1, padding=0),\n            #nn.ReLU()\n        )\n\n\n    def forward(self,img_tensor):\n        x1 = self.column1(img_tensor)\n        x2 = self.column2(img_tensor)\n        x3 = self.column3(img_tensor)\n        x = torch.cat((x1, x2, x3),1)\n        x = self.fusion_layer(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:27:16.527761Z","iopub.execute_input":"2025-10-24T07:27:16.528257Z","iopub.status.idle":"2025-10-24T07:27:16.537755Z","shell.execute_reply.started":"2025-10-24T07:27:16.528233Z","shell.execute_reply":"2025-10-24T07:27:16.536944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:27:16.538394Z","iopub.execute_input":"2025-10-24T07:27:16.538561Z","iopub.status.idle":"2025-10-24T07:27:20.610073Z","shell.execute_reply.started":"2025-10-24T07:27:16.538548Z","shell.execute_reply":"2025-10-24T07:27:20.609333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1 batch size, 3 color channels (RGB), 768x1024 image dimensions\nimg = torch.rand((1, 3, 768, 1024), dtype=torch.float)\n\n# Initialize the MC-CNN model\nmcnn = MC_CNN()\n\n# Generate the density map using the model\nout_dmap = mcnn(img)\n\n# Print the shape of the output density map\nprint(out_dmap.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:27:20.611182Z","iopub.execute_input":"2025-10-24T07:27:20.611502Z","iopub.status.idle":"2025-10-24T07:27:21.298852Z","shell.execute_reply.started":"2025-10-24T07:27:20.611481Z","shell.execute_reply":"2025-10-24T07:27:21.298140Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchsummary import summary\n\n# Select device (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move model and sample input to the selected device\nmcnn = mcnn.to(device)\nimg = img.to(device)\n\n# Display model summary\nsummary(mcnn, input_size=(3, 768, 1024))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:27:21.299722Z","iopub.execute_input":"2025-10-24T07:27:21.300131Z","iopub.status.idle":"2025-10-24T07:27:22.369179Z","shell.execute_reply.started":"2025-10-24T07:27:21.300104Z","shell.execute_reply":"2025-10-24T07:27:22.368472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Subset, random_split\n\nbatch_size = 8\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ntrain_root_dir = \"../input/shanghaitech/ShanghaiTech/part_B/train_data/\"\ntest_root_dir = \"../input/shanghaitech/ShanghaiTech/part_B/test_data/\"\n\n# Initialize dataset (no shuffle here)\nfull_train_dataset = CrowdDataset(train_root_dir, gt_downsample=4)\n\n# Split 90% for training and 10% for validation\ntrain_size = int(0.9 * len(full_train_dataset))\nval_size = len(full_train_dataset) - train_size\ntrain_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n\n# Create DataLoaders (shuffle only for training)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# Test dataset and loader\ntest_dataset = CrowdDataset(test_root_dir, gt_downsample=4)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Print stats\nprint(\"Number of images in train_dataset:\", len(train_dataset))\nprint(\"Number of images in val_dataset:\", len(val_dataset))\nprint(\"Number of images in test_dataset:\", len(test_dataset))\nprint(\"Number of batches in train_loader:\", len(train_loader))\nprint(\"Number of batches in val_loader:\", len(val_loader))\nprint(\"Number of batches in test_loader:\", len(test_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:27:22.369981Z","iopub.execute_input":"2025-10-24T07:27:22.370244Z","iopub.status.idle":"2025-10-24T07:30:34.660415Z","shell.execute_reply.started":"2025-10-24T07:27:22.370221Z","shell.execute_reply":"2025-10-24T07:30:34.659696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize pairs of images and their corresponding density maps side by side in a grid layout\ndef plot_corresponding_pairs(batch1, batch2, plot_map='jet'):\n    num_images = batch1.shape[0]\n    num_cols = 4\n    num_rows = int(np.ceil(num_images / num_cols)) * 2  # two rows per image (image + map)\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(16, num_rows * 2))\n    axes = axes.flatten()  # flatten for easier indexing\n\n    for i in range(num_images):\n        # Plot image\n        axes[i * 2].imshow(batch1[i].permute(1, 2, 0).cpu())\n        axes[i * 2].axis('off')\n\n        # Plot corresponding density map\n        dmap = batch2[i].squeeze().detach().cpu().numpy()\n        axes[i * 2 + 1].imshow(dmap, cmap=plot_map)\n        axes[i * 2 + 1].set_title(f\"DM: {np.sum(dmap):.2f}\")\n        axes[i * 2 + 1].axis('off')\n\n    # Hide unused subplots\n    for j in range(i * 2 + 2, len(axes)):\n        axes[j].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:30:34.661150Z","iopub.execute_input":"2025-10-24T07:30:34.661382Z","iopub.status.idle":"2025-10-24T07:30:34.667214Z","shell.execute_reply.started":"2025-10-24T07:30:34.661364Z","shell.execute_reply":"2025-10-24T07:30:34.666560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get one batch of samples from the training loader\ndata_iter = iter(train_loader)\nsample_images, sample_dmaps, sample_n_people = next(data_iter)\n\n# Visualize images and their corresponding density maps\nplot_corresponding_pairs(sample_images, sample_dmaps)\n\n# Print ground truth number of people for each sample\nprint(\"Ground truth counts:\")\nprint(\" \".join(f\"{sample_n_people[j].item():5.1f}\" for j in range(batch_size)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:30:34.668086Z","iopub.execute_input":"2025-10-24T07:30:34.668733Z","iopub.status.idle":"2025-10-24T07:30:36.647298Z","shell.execute_reply.started":"2025-10-24T07:30:34.668710Z","shell.execute_reply":"2025-10-24T07:30:36.646656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CombinedLoss(nn.Module):\n    def __init__(self, weight_dmap=0.8, weight_sum_gt=0.2):\n        super().__init__()\n        self.weight_dmap = weight_dmap\n        self.weight_sum_gt = weight_sum_gt\n\n        self.img_loss = nn.MSELoss()\n        self.gt_loss_mae = nn.L1Loss()\n        self.gt_loss_mse = nn.MSELoss()  \n\n    def forward(self, logits, batch_dmap, batch_gts):\n        batch_gts = batch_gts.float()\n\n        # Density map loss (pixel-level MSE)\n        img_loss = self.img_loss(logits, batch_dmap)\n\n        # Count-level losses (sum over density map)\n        pred_counts = torch.squeeze(logits.sum(dim=(2, 3)))\n        gt_loss_mae = self.gt_loss_mae(pred_counts, batch_gts)\n        gt_loss_mse = self.gt_loss_mse(pred_counts, batch_gts)\n\n        # Weighted combined loss\n        combined_loss = (\n            self.weight_dmap * img_loss +\n            self.weight_sum_gt * gt_loss_mae\n        )\n\n        return combined_loss, gt_loss_mae\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:30:36.648088Z","iopub.execute_input":"2025-10-24T07:30:36.648413Z","iopub.status.idle":"2025-10-24T07:30:36.656177Z","shell.execute_reply.started":"2025-10-24T07:30:36.648393Z","shell.execute_reply":"2025-10-24T07:30:36.655061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 50\n\n# Track losses\ntrain_losses, val_losses = [], []\ntrain_mae_losses, val_mae_losses = [], []\n\n# Initialize model, criterion, optimizer\nmodel = MC_CNN().to(device)\ncriterion = CombinedLoss(weight_dmap=0.8, weight_sum_gt=0.2)\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\nbest_val_loss = float('inf')\nbest_epoch = 0\n\nfor epoch in range(num_epochs):\n    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n\n\n    # Training Phase\n    model.train()\n    tr_loss_acc, tr_mae_acc = 0.0, 0.0\n\n    for batch_img, batch_dmap, batch_gts in train_loader:\n        batch_img, batch_dmap, batch_gts = (\n            batch_img.to(device),\n            batch_dmap.to(device),\n            batch_gts.to(device)\n        )\n\n        # Forward pass\n        logits = model(batch_img)\n        loss, mae_loss = criterion(logits, batch_dmap, batch_gts)\n\n        # Backward pass and optimizer step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Accumulate losses\n        tr_loss_acc += loss.item()\n        tr_mae_acc += mae_loss.item()\n\n    # Compute average over the dataset\n    tr_loss = tr_loss_acc / len(train_loader.dataset)\n    tr_mae = tr_mae_acc / len(train_loader.dataset)\n    print(f\">> TRAIN | Loss: {tr_loss:.6f} | MAE: {tr_mae:.6f}\")\n\n  \n    # Validation Phase\n    model.eval()\n    val_loss_acc, val_mae_acc = 0.0, 0.0\n\n    with torch.inference_mode():\n        for batch_img_val, batch_dmap_val, batch_gts_val in val_loader:\n            batch_img_val, batch_dmap_val, batch_gts_val = (\n                batch_img_val.to(device),\n                batch_dmap_val.to(device),\n                batch_gts_val.to(device)\n            )\n\n            logits = model(batch_img_val)\n            loss, mae_loss = criterion(logits, batch_dmap_val, batch_gts_val)\n\n            val_loss_acc += loss.item()\n            val_mae_acc += mae_loss.item()\n\n    val_loss = val_loss_acc / len(val_loader.dataset)\n    val_mae = val_mae_acc / len(val_loader.dataset)\n    print(f\">> VAL   | Loss: {val_loss:.6f} | MAE: {val_mae:.6f}\")\n\n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_epoch = epoch\n        torch.save(model.state_dict(), './crowd_counting_best.pth')\n\n    # Track losses\n    train_losses.append(tr_loss)\n    train_mae_losses.append(tr_mae)\n    val_losses.append(val_loss)\n    val_mae_losses.append(val_mae)\n\nprint(f\"\\nBest Epoch: {best_epoch + 1}\")\nprint(f\"Best TRAIN MAE: {train_mae_losses[best_epoch]:.6f}\")\nprint(f\"Best VAL   MAE: {val_mae_losses[best_epoch]:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T07:30:36.657086Z","iopub.execute_input":"2025-10-24T07:30:36.657397Z","iopub.status.idle":"2025-10-24T08:04:46.786797Z","shell.execute_reply.started":"2025-10-24T07:30:36.657376Z","shell.execute_reply":"2025-10-24T08:04:46.786142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\n# Weighted Loss Plot\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Weighted Loss', marker='o')\nplt.plot(val_losses, label='Validation Weighted Loss', marker='o')\nplt.title('Training vs Validation LOSS')\nplt.xlabel('Epochs')\nplt.ylabel('Weighted Loss')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\n# MAE Plot\nplt.subplot(1, 2, 2)\nplt.plot(train_mae_losses, label='Training MAE', marker='o')\nplt.plot(val_mae_losses, label='Validation MAE', marker='o')\nplt.title('Training vs Validation MAE')\nplt.xlabel('Epochs')\nplt.ylabel('Mean Absolute Error (MAE)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T08:04:46.787638Z","iopub.execute_input":"2025-10-24T08:04:46.788245Z","iopub.status.idle":"2025-10-24T08:04:47.149016Z","shell.execute_reply.started":"2025-10-24T08:04:46.788225Z","shell.execute_reply":"2025-10-24T08:04:47.148314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the best saved model\nbest_model = MC_CNN().to(device)\nbest_model.load_state_dict(torch.load('./crowd_counting_best.pth', map_location=device))\nbest_model.eval()\n\n# Get a random validation batch\ndata_iter = iter(val_loader)\nsample_images, _, sample_gts = next(data_iter)\n\n# Predict density maps\nwith torch.inference_mode():\n    pred_dms = best_model(sample_images.to(device))\n\n# Visualize original images and their predicted density maps\nplot_corresponding_pairs(sample_images.cpu(), pred_dms.cpu(), plot_map='twilight')\n\n# Print ground-truth counts for reference\nprint(\"Ground Truth Counts:\")\nprint(\" \".join(f\"{sample_gts[j].item():5.1f}\" for j in range(batch_size)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T08:08:00.305609Z","iopub.execute_input":"2025-10-24T08:08:00.305891Z","iopub.status.idle":"2025-10-24T08:08:02.386014Z","shell.execute_reply.started":"2025-10-24T08:08:00.305873Z","shell.execute_reply":"2025-10-24T08:08:02.385188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Mean Absolute Error (MAE) for crowd counting\ncriterion = nn.L1Loss()\n\ntest_loss_acc = 0.0\n\n# Disable gradient tracking for evaluation\nwith torch.inference_mode():\n    for batch_img, batch_dmap, batch_gts in test_loader:\n        batch_img = batch_img.to(device)\n        batch_dmap = batch_dmap.to(device)\n        batch_gts = batch_gts.to(device)\n\n        # Predict and compute MAE between predicted and true counts\n        logits = best_model(batch_img)\n        pred_counts = torch.squeeze(logits.sum(dim=(2, 3)))\n        loss = criterion(pred_counts, batch_gts)\n\n        test_loss_acc += loss.item()\n\n# Print test results\ntest_mae = test_loss_acc / len(test_loader.dataset)\nprint(f\"TEST:  MAE = {test_mae:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T08:08:27.675403Z","iopub.execute_input":"2025-10-24T08:08:27.675895Z","iopub.status.idle":"2025-10-24T08:08:42.296707Z","shell.execute_reply.started":"2025-10-24T08:08:27.675865Z","shell.execute_reply":"2025-10-24T08:08:42.295984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate MAE and RMSE on the test set\nfrom math import sqrt\n\nbest_model.eval()\nmae_total, mse_total = 0.0, 0.0\n\nwith torch.inference_mode():\n    for batch_img, _, batch_gts in test_loader:\n        batch_img, batch_gts = batch_img.to(device), batch_gts.to(device)\n        logits = best_model(batch_img)\n\n        pred_counts = torch.squeeze(logits.sum(dim=(2, 3)))\n        diff = pred_counts - batch_gts\n        mae_total += torch.abs(diff).sum().item()\n        mse_total += (diff ** 2).sum().item()\n\nnum_samples = len(test_loader.dataset)\nmae = mae_total / num_samples\nrmse = sqrt(mse_total / num_samples)\n\nprint(f\"TEST RESULTS:\")\nprint(f\"  MAE  = {mae:.3f}\")\nprint(f\"  RMSE = {rmse:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T08:08:50.102457Z","iopub.execute_input":"2025-10-24T08:08:50.102958Z","iopub.status.idle":"2025-10-24T08:09:04.134769Z","shell.execute_reply.started":"2025-10-24T08:08:50.102933Z","shell.execute_reply":"2025-10-24T08:09:04.134015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_iter = iter(test_loader)\nsample_images, _, sample_gts = next(data_iter)\nsample_images = sample_images.to(device)\n\nwith torch.inference_mode():\n    pred_dmaps = best_model(sample_images)\n    pred_counts = pred_dmaps.sum(dim=(2, 3)).cpu().numpy().flatten()\n\nplot_corresponding_pairs(sample_images.cpu(), pred_dmaps.cpu(), plot_map='viridis')\n\nprint(\"Ground Truth Counts: \", sample_gts.tolist())\nprint(\"Predicted Counts:    \", [round(x, 1) for x in pred_counts])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T08:09:43.159640Z","iopub.execute_input":"2025-10-24T08:09:43.160223Z","iopub.status.idle":"2025-10-24T08:09:45.319203Z","shell.execute_reply.started":"2025-10-24T08:09:43.160197Z","shell.execute_reply":"2025-10-24T08:09:45.318562Z"}},"outputs":[],"execution_count":null}]}